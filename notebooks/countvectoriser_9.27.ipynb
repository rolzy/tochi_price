{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_array\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    #y_true, y_pred = check_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 販売数は 6461 件、特徴量は 158 個。\n",
      "Test: 販売数は 4273 件、特徴量は 157 個。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tochi_train = pd.read_csv('../data/train_genba.tsv', sep='\\t')\n",
    "build_train = pd.read_csv('../data/train_goto.tsv', sep='\\t')\n",
    "train = pd.merge(tochi_train, build_train, on=\"pj_no\")\n",
    "\n",
    "tochi_test = pd.read_csv('../data/test_genba.tsv', sep='\\t')\n",
    "build_test = pd.read_csv('../data/test_goto.tsv', sep='\\t')\n",
    "test = pd.merge(tochi_test, build_test, on=\"pj_no\")\n",
    "\n",
    "print (\"Train: 販売数は\",train.shape[0],\"件、特徴量は\",train.shape[1],\"個。\")\n",
    "print (\"Test: 販売数は\",test.shape[0],\"件、特徴量は\",test.shape[1],\"個。\")\n",
    "\n",
    "first_submission = pd.DataFrame()\n",
    "first_submission['id'] = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前系と相関性が高いカラムはとりあえず削除\n",
    "# 土地のネームバリューが出てくると思うので、名前系はあとで追加するかも\n",
    "\n",
    "name_columns = ['bastei_nm1','bastei_nm2','chiseki_kb_hb','eki_nm1','eki_nm2','gk_chu_tm','gk_sho_tm','hy1f_date_su', \\\n",
    "                'hy2f_date_su','mseki_yt_hb','tc_mseki','yoseki2','id']\n",
    "train.drop(name_columns, axis=1, inplace=True)\n",
    "test.drop(name_columns, axis=1, inplace=True)\n",
    "\n",
    "y_train = train['keiyaku_pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ系コラムにある「無」に値を振り分けたり、変換ミスに対処したり、マルバツからBooleanに変換\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train['fi3m_yohi'].replace('（無）','（不要）',inplace=True)\n",
    "train['hiatari'].fillna('普通', inplace=True)\n",
    "train['kborjs'].replace('公募','公簿',inplace=True)\n",
    "test['fi3m_yohi'].replace('（無）','（不要）',inplace=True)\n",
    "test['hiatari'].fillna('普通', inplace=True)\n",
    "test['kborjs'].replace('公募','公簿',inplace=True)\n",
    "\n",
    "maru_columns = ['rs_e_kdate2','rs_e_kdate3','rs_e_m_ari','rs_e_m_nashi','rs_e_parking','rs_e_tahata','rs_e_zoki', \\\n",
    "                'rs_n_kdate2','rs_n_kdate3','rs_n_m_ari','rs_n_m_nashi','rs_n_parking','rs_n_tahata','rs_n_zoki', \\\n",
    "                'rs_s_kdate2','rs_s_kdate3','rs_s_m_ari','rs_s_m_nashi','rs_s_parking','rs_s_tahata','rs_s_zoki', \\\n",
    "                'rs_w_kdate2','rs_w_kdate3','rs_w_m_ari','rs_w_m_nashi','rs_w_parking','rs_w_tahata','rs_w_zoki', \\\n",
    "                'sho_conv','sho_market','sho_shoten','sho_super','shu_bochi','shu_factory','shu_highway', \\\n",
    "                'shu_hvline','shu_jutaku','shu_kaido','shu_kokyo','shu_line_ari','shu_line_nashi','shu_park', \\\n",
    "                'shu_shop','shu_sogi','shu_soon','shu_tower','shu_zoki']\n",
    "\n",
    "train[maru_columns] = train[maru_columns].replace({'○':1, np.nan:0})\n",
    "test[maru_columns] = test[maru_columns].replace({'○':1, np.nan:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 他規制や個別要因など、「複数ある場合は1～4」系のカラムに対処\n",
    "\n",
    "hokakisei=['hokakisei1','hokakisei2','hokakisei3','hokakisei4']\n",
    "kobetsu=['kobetsu1','kobetsu2','kobetsu3','kobetsu4']\n",
    "\n",
    "train = pd.concat([train, train[hokakisei].stack().str.get_dummies().sum(level=0), \\\n",
    "                train[kobetsu].stack().str.get_dummies().sum(level=0)], axis=1)\n",
    "train.drop(hokakisei+kobetsu, axis=1, inplace=True)\n",
    "train.iloc[:,137:] = train.iloc[:,137:].fillna(0.0).apply(lambda x: [0 if y == 0.0 else 1 for y in x])\n",
    "test = pd.concat([test, test[hokakisei].stack().str.get_dummies().sum(level=0), \\\n",
    "                test[kobetsu].stack().str.get_dummies().sum(level=0)], axis=1)\n",
    "test.drop(hokakisei+kobetsu, axis=1, inplace=True)\n",
    "test.iloc[:,136:] = test.iloc[:,136:].fillna(0.0).apply(lambda x: [0 if y == 0.0 else 1 for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BooleanであるハズがCategoricalになってるカラムに対処\n",
    "\n",
    "bool_columns = ['bus_yohi','chikukeikaku','fi3m_yohi','fi4m_yohi','gesui','hokakyoka','josui','kaihatsukyoka','kaoku_um', \\\n",
    "                'kborjs','keikakuroad','kinshijiko','t53kyoka','yheki_umu','yheki_yohi']\n",
    "\n",
    "train[bool_columns] = train[bool_columns].replace({'（不要）':0, '（無）':0,'（要）':1,'（有）':1,'公共下水':0,'個別浄化槽':1,\\\n",
    "                                                   '公営':0,'私営':1,'実測':0,'公簿':1})\n",
    "test[bool_columns] = test[bool_columns].replace({'（不要）':0, '（無）':0,'（要）':1,'（有）':1,'公共下水':0,'個別浄化槽':1,\\\n",
    "                                                 '公営':0,'私営':1,'実測':0,'公簿':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ちょっとデータを追加\n",
    "# Levelplanから階数と部屋を分割\n",
    "\n",
    "levelplan_split_train = train['levelplan'].str.split('/', n=1, expand=True)\n",
    "train['level'] = levelplan_split_train[0]\n",
    "train['rooms'] = levelplan_split_train[1]\n",
    "\n",
    "levelplan_split_test = test['levelplan'].str.split('/', n=1, expand=True)\n",
    "test['level'] = levelplan_split_test[0]\n",
    "test['rooms'] = levelplan_split_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経度を利用する\n",
    "# そのままではvarianceが低すぎるので、QuantileTransformerを利用する。\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "latlon_dic = dict()\n",
    "with open('longlat.txt','r') as f:\n",
    "    for line in f:\n",
    "        addr = line.replace('\\n','').split(',')[0]\n",
    "        latlon = line.replace('\\n','').split(',')[1:]\n",
    "        latlon_dic.update({addr:latlon})\n",
    "\n",
    "def get_lon(addr):\n",
    "    try:\n",
    "        return float(latlon_dic.get(addr)[1])\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "\n",
    "train['lat'] = train['jukyo'].apply(lambda x: float(latlon_dic.get(x)[0]))\n",
    "train['lon'] = train['jukyo'].apply(lambda x: get_lon(x))\n",
    "test['lat'] = test['jukyo'].apply(lambda x: float(latlon_dic.get(x)[0]))\n",
    "test['lon'] = test['jukyo'].apply(lambda x: get_lon(x))\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "train[['lat','lon']] = scaler.fit_transform(train[['lat','lon']])\n",
    "test[['lat','lon']] = scaler.transform(test[['lat','lon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析\n",
    "\n",
    "import collections\n",
    "import MeCab\n",
    "import mojimoji\n",
    "from string import digits\n",
    "\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "tagger = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\")\n",
    "\n",
    "def extract_words(line):\n",
    "    keyword=[]\n",
    "    node = tagger.parseToNode(line).next\n",
    "    while node:\n",
    "        keyword.append(node.surface)\n",
    "        node = node.next\n",
    "    keyword = str(keyword).replace(\"', '\",\" \")\n",
    "    keyword = keyword.replace(\"\\'\",\"\")\n",
    "    keyword = keyword.replace(\"[\",\"\")\n",
    "    keyword = keyword.replace(\"]\",\"\")\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部埼玉県なので住居から消去\n",
    "# あと市が抜けてる住所情報は追加してあげる\n",
    "\n",
    "shi_gun_dic = dict({'にっさい花みず木':'坂戸市にっさい花みず木','西鶴ヶ岡':'ふじみ野市西鶴ヶ岡', \\\n",
    "                    '杉戸町内田':'北葛飾郡杉戸町内田','宮代町宮代台':'南埼玉郡宮代町宮代台', \\\n",
    "                    '大字下日出谷':'桶川市大字下日出谷','杉戸町清地':'北葛飾郡杉戸町', \\\n",
    "                    '松伏町田中':'北葛飾郡松伏町','大字水野字逃水':'狭山市大字水野字逃水'})\n",
    "\n",
    "train['jukyo'] = train['jukyo'].str.replace('埼玉県','')\n",
    "test['jukyo'] = test['jukyo'].str.replace('埼玉県','')\n",
    "train['jukyo'] = train['jukyo'].replace(shi_gun_dic)\n",
    "test['jukyo'] = test['jukyo'].replace(shi_gun_dic)\n",
    "\n",
    "jukyo_split_train = train['jukyo'].str.split(r'市|郡', n=1, expand=True)\n",
    "train['jukyo_shi_gun'] = jukyo_split_train[0]\n",
    "train['town'] = jukyo_split_train[1]\n",
    "train.drop('jukyo', axis=1, inplace=True)\n",
    "\n",
    "jukyo_split_test = test['jukyo'].str.split(r'市|郡', n=1, expand=True)\n",
    "test['jukyo_shi_gun'] = jukyo_split_test[0]\n",
    "test['town'] = jukyo_split_test[1]\n",
    "test['town'].fillna('無', inplace=True)\n",
    "test.drop('jukyo', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# word count feature\n",
    "cv_shi_gun = CountVectorizer()\n",
    "cv_town = CountVectorizer()\n",
    "\n",
    "train_cv_shi = cv_shi_gun.fit_transform(train.jukyo_shi_gun)\n",
    "train_cv_shi = pd.DataFrame(train_cv_shi.toarray(),columns=cv_shi_gun.get_feature_names())\n",
    "test_cv_shi = cv_shi_gun.transform(test.jukyo_shi_gun)\n",
    "test_cv_shi = pd.DataFrame(test_cv_shi.toarray(),columns=cv_shi_gun.get_feature_names())\n",
    "\n",
    "train_cv_town = cv_town.fit_transform(train.town)\n",
    "train_cv_town = pd.DataFrame(train_cv_town.toarray(),columns=cv_town.get_feature_names())\n",
    "test_cv_town = cv_town.transform(test.town)\n",
    "test_cv_town = pd.DataFrame(test_cv_town.toarray(),columns=cv_town.get_feature_names())\n",
    "\n",
    "train.drop('town', axis=1, inplace=True)\n",
    "test.drop('town', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6461, 1370)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, train_cv_shi, train_cv_town], axis=1)\n",
    "test = pd.concat([test, test_cv_shi, test_cv_town], axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-3b376764124c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 数値とカテゴリに分ける\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquantitative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mquantitative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keiyaku_pr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquantitative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pj_no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-3b376764124c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 数値とカテゴリに分ける\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquantitative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mquantitative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keiyaku_pr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquantitative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pj_no'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/kaggle/tochi_price/env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1477\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# 数値とカテゴリに分ける\n",
    "\n",
    "quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "quantitative.remove('keiyaku_pr')\n",
    "quantitative.remove('pj_no')\n",
    "qualitative = [f for f in train.columns if train.dtypes[f] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNが多いフィーチャーを表示\n",
    "\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "missing = train.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータセットでは、メインとサブのフィーチャーが多い。（最寄り駅1、最寄り駅2など）\n",
    "最寄り駅2がない家は、それに該当するフィーチャー（名前や距離など）がNaNになることが多い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "plt.figure(1); plt.title('Johnson SU')\n",
    "sns.distplot(y_train, kde=False, fit=stats.johnsonsu)\n",
    "plt.figure(2); plt.title('Normal')\n",
    "sns.distplot(y_train, kde=False, fit=stats.norm)\n",
    "plt.figure(3); plt.title('Log Normal')\n",
    "sns.distplot(y_train, kde=False, fit=stats.lognorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "契約価格は、すでにNormal Distributionに近いため、調整は必要なし。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitative情報の、各カテゴリの契約価格平均を割り当てて、それでcorrelationを測る\n",
    "\n",
    "def encode(frame, feature):\n",
    "    ordering = pd.DataFrame()\n",
    "    ordering['val'] = frame[feature].unique()\n",
    "    ordering.index = ordering.val\n",
    "    ordering['spmean'] = frame[[feature, 'keiyaku_pr']].groupby(feature).mean()['keiyaku_pr']\n",
    "    ordering = ordering.sort_values('spmean')\n",
    "    ordering['ordering'] = range(1, ordering.shape[0]+1)\n",
    "    ordering = ordering['ordering'].to_dict()\n",
    "    \n",
    "    for cat, o in ordering.items():\n",
    "        frame.loc[frame[feature] == cat, feature+'_E'] = o\n",
    "    \n",
    "qual_encoded = []\n",
    "for q in qualitative:  \n",
    "    encode(train, q)\n",
    "    qual_encoded.append(q+'_E')\n",
    "print(qual_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Measure strength and direction of monotonic (linear) relationship\n",
    "def spearman(frame, features):\n",
    "    spr = pd.DataFrame()\n",
    "    spr['feature'] = features\n",
    "    spr['spearman'] = [frame[f].corr(frame['keiyaku_pr'], 'spearman') for f in features]\n",
    "    spr = spr.sort_values('spearman')\n",
    "    plt.figure(figsize=(6, 0.25*len(features)))\n",
    "    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n",
    "    \n",
    "features = quantitative + qual_encoded\n",
    "spearman(train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "corr = train[quantitative+['keiyaku_pr']].corr()\n",
    "sns.heatmap(corr)\n",
    "plt.figure(2)\n",
    "corr = train[qual_encoded+['keiyaku_pr']].corr()\n",
    "sns.heatmap(corr)\n",
    "plt.figure(3)\n",
    "corr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['keiyaku_pr'], columns=qual_encoded+['SalePrice'])\n",
    "for q1 in quantitative+['keiyaku_pr']:\n",
    "    for q2 in qual_encoded+['keiyaku_pr']:\n",
    "        corr.loc[q1, q2] = train[q1].corr(train[q2])\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後に、categoricalなカラムを全てone_hot_encode\n",
    "\n",
    "categorical = ['bas_toho1','bas_toho2','bokachiiki','gas','hiatari','hw_status','jigata','kodochiku','levelplan', \\\n",
    "               'road1_hk','road1_sb','road2_hk','road2_sb','road3_sb','road3_hk','road4_sb','road4_hk','road_st', \\\n",
    "               'rosen_nm1','rosen_nm2','setsudo_hi','setsudo_kj','toshikuiki1','toshikuiki2','usui','yoto1','yoto2', \\\n",
    "               'jukyo_shi_gun','level','rooms']\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(train[categorical])], axis=1)\n",
    "train.drop(categorical, axis=1, inplace=True)\n",
    "test = pd.concat([test, pd.get_dummies(test[categorical])], axis=1)\n",
    "test.drop(categorical, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 両方のDataframeに登場しないカラムを除外（価格はキープしとく）\n",
    "\n",
    "train_columns = list(train.columns.values)\n",
    "test_columns = list(test.columns.values)\n",
    "unique_columns = list(set(train_columns) ^ set(test_columns))\n",
    "\n",
    "train.drop(unique_columns, axis=1, inplace=True, errors='ignore')\n",
    "test.drop(unique_columns, axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6461, 1597)\n",
      "(4273, 1597)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold1 start\n",
      "fold1 end\n",
      "Accuracy = 9.40647613601396\n",
      "fold2 start\n",
      "fold2 end\n",
      "Accuracy = 9.33552241911634\n",
      "fold3 start\n",
      "fold3 end\n",
      "Accuracy = 9.03553395287467\n",
      "fold4 start\n",
      "fold4 end\n",
      "Accuracy = 11.371655392825806\n",
      "fold5 start\n",
      "fold5 end\n",
      "Accuracy = 9.88582601353566\n",
      "[9.40647613601396, 9.33552241911634, 9.03553395287467, 11.371655392825806, 9.88582601353566] 平均score 9.807002782873289\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 3分割交差検証を指定し、インスタンス化 \n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "# スコアとモデルを格納するリスト \n",
    "score_list = [] \n",
    "models = [] \n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "for fold_, (train_index, valid_index) in enumerate(kf.split(train, y_train)):\n",
    "    train_x = train.iloc[train_index]\n",
    "    valid_x = train.iloc[valid_index]\n",
    "    train_y = y_train[train_index]\n",
    "    valid_y = y_train[valid_index]\n",
    "    \n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_x, train_y)\n",
    "    lgb_eval = lgb.Dataset(valid_x, valid_y, reference=lgb_train)\n",
    "    print(f'fold{fold_ + 1} start')\n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=5000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=0)\n",
    "    y_pred = gbm.predict(valid_x, num_iteration=gbm.best_iteration)\n",
    "    score_list.append(mean_absolute_percentage_error(valid_y, y_pred))\n",
    "    models.append(gbm)  # 学習が終わったモデルをリストに入れておく\n",
    "    print(f'fold{fold_ + 1} end\\nAccuracy = {mean_absolute_percentage_error(valid_y, y_pred)}')\n",
    "print(score_list, '平均score', np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.zeros((len(test), 5)) \n",
    "for fold_, gbm in enumerate(models):\n",
    "    pred_ = gbm.predict(test, num_iteration=gbm.best_iteration)# testを予測\n",
    "    test_pred[:, fold_] = pred_ \n",
    "pred = np.mean(test_pred, axis=1)\n",
    "first_submission['price'] = pred\n",
    "first_submission.to_csv('lightgbm.tsv', sep='\\t', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
