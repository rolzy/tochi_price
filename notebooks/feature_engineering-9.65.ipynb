{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_array\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    #y_true, y_pred = check_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tochi_train = pd.read_csv('../data/train_genba.tsv', sep='\\t')\n",
    "build_train = pd.read_csv('../data/train_goto.tsv', sep='\\t')\n",
    "train = pd.merge(tochi_train, build_train, on=\"pj_no\")\n",
    "\n",
    "tochi_test = pd.read_csv('../data/test_genba.tsv', sep='\\t')\n",
    "build_test = pd.read_csv('../data/test_goto.tsv', sep='\\t')\n",
    "test = pd.merge(tochi_test, build_test, on=\"pj_no\")\n",
    "\n",
    "first_submission = pd.DataFrame()\n",
    "first_submission['id'] = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前系と相関性が高いカラムはとりあえず削除\n",
    "# 土地のネームバリューが出てくると思うので、名前系はあとで追加するかも\n",
    "\n",
    "name_columns = ['bastei_nm1','bastei_nm2','chiseki_kb_hb','eki_nm1','eki_nm2','gk_chu_tm','gk_sho_tm','hy1f_date_su', \\\n",
    "                'hy2f_date_su','mseki_yt_hb','tc_mseki','yoseki2','id']\n",
    "train.drop(name_columns, axis=1, inplace=True)\n",
    "test.drop(name_columns, axis=1, inplace=True)\n",
    "\n",
    "y_train = train['keiyaku_pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ系コラムにある「無」に値を振り分けたり、変換ミスに対処したり、マルバツからBooleanに変換\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train['fi3m_yohi'].replace('（無）','（不要）',inplace=True)\n",
    "train['hiatari'].fillna('普通', inplace=True)\n",
    "train['kborjs'].replace('公募','公簿',inplace=True)\n",
    "test['fi3m_yohi'].replace('（無）','（不要）',inplace=True)\n",
    "test['hiatari'].fillna('普通', inplace=True)\n",
    "test['kborjs'].replace('公募','公簿',inplace=True)\n",
    "\n",
    "maru_columns = ['rs_e_kdate2','rs_e_kdate3','rs_e_m_ari','rs_e_m_nashi','rs_e_parking','rs_e_tahata','rs_e_zoki', \\\n",
    "                'rs_n_kdate2','rs_n_kdate3','rs_n_m_ari','rs_n_m_nashi','rs_n_parking','rs_n_tahata','rs_n_zoki', \\\n",
    "                'rs_s_kdate2','rs_s_kdate3','rs_s_m_ari','rs_s_m_nashi','rs_s_parking','rs_s_tahata','rs_s_zoki', \\\n",
    "                'rs_w_kdate2','rs_w_kdate3','rs_w_m_ari','rs_w_m_nashi','rs_w_parking','rs_w_tahata','rs_w_zoki', \\\n",
    "                'sho_conv','sho_market','sho_shoten','sho_super','shu_bochi','shu_factory','shu_highway', \\\n",
    "                'shu_hvline','shu_jutaku','shu_kaido','shu_kokyo','shu_line_ari','shu_line_nashi','shu_park', \\\n",
    "                'shu_shop','shu_sogi','shu_soon','shu_tower','shu_zoki']\n",
    "\n",
    "train[maru_columns] = train[maru_columns].replace({'○':1, np.nan:0})\n",
    "test[maru_columns] = test[maru_columns].replace({'○':1, np.nan:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 他規制や個別要因など、「複数ある場合は1～4」系のカラムに対処\n",
    "\n",
    "hokakisei=['hokakisei1','hokakisei2','hokakisei3','hokakisei4']\n",
    "kobetsu=['kobetsu1','kobetsu2','kobetsu3','kobetsu4']\n",
    "\n",
    "train = pd.concat([train, train[hokakisei].stack().str.get_dummies().sum(level=0), \\\n",
    "                train[kobetsu].stack().str.get_dummies().sum(level=0)], axis=1)\n",
    "train.drop(hokakisei+kobetsu, axis=1, inplace=True)\n",
    "train.iloc[:,136:] = train.iloc[:,136:].fillna(0.0).apply(lambda x: [0 if y == 0.0 else 1 for y in x])\n",
    "test = pd.concat([test, test[hokakisei].stack().str.get_dummies().sum(level=0), \\\n",
    "                test[kobetsu].stack().str.get_dummies().sum(level=0)], axis=1)\n",
    "test.drop(hokakisei+kobetsu, axis=1, inplace=True)\n",
    "test.iloc[:,135:] = test.iloc[:,135:].fillna(0.0).apply(lambda x: [0 if y == 0.0 else 1 for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BooleanであるハズがCategoricalになってるカラムに対処\n",
    "\n",
    "bool_columns = ['bus_yohi','chikukeikaku','fi3m_yohi','fi4m_yohi','gesui','hokakyoka','josui','kaihatsukyoka','kaoku_um', \\\n",
    "                'kborjs','keikakuroad','kinshijiko','t53kyoka','yheki_umu','yheki_yohi']\n",
    "\n",
    "train[bool_columns] = train[bool_columns].replace({'（不要）':0, '（無）':0,'（要）':1,'（有）':1,'公共下水':0,'個別浄化槽':1,\\\n",
    "                                                   '公営':0,'私営':1,'実測':0,'公簿':1})\n",
    "test[bool_columns] = test[bool_columns].replace({'（不要）':0, '（無）':0,'（要）':1,'（有）':1,'公共下水':0,'個別浄化槽':1,\\\n",
    "                                                 '公営':0,'私営':1,'実測':0,'公簿':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ちょっとデータを追加\n",
    "# Levelplanから階数と部屋を分割\n",
    "# 公表された平均価格の平均を追加\n",
    "\n",
    "levelplan_split_train = train['levelplan'].str.split('/', n=1, expand=True)\n",
    "train['level'] = levelplan_split_train[0]\n",
    "train['rooms'] = levelplan_split_train[1]\n",
    "\n",
    "levelplan_split_test = test['levelplan'].str.split('/', n=1, expand=True)\n",
    "test['level'] = levelplan_split_test[0]\n",
    "test['rooms'] = levelplan_split_test[1]\n",
    "\n",
    "train['avgPrice'] = (train['koji_hb']+train['kijun_hb']+train['rosenka_hb'])/3 \n",
    "test['avgPrice'] = (test['koji_hb']+test['kijun_hb']+test['rosenka_hb'])/3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['koji_hb','avgPrice','kijun_hb','rosenka_hb']\n",
    "\n",
    "train[\"koji_hb-s2\"] = train[\"koji_hb\"] ** 2\n",
    "train[\"koji_hb-s3\"] = train[\"koji_hb\"] ** 3\n",
    "train[\"koji_hb-Sq\"] = np.sqrt(train[\"koji_hb\"])\n",
    "train[\"avgPrice-s2\"] = train[\"avgPrice\"] ** 2\n",
    "train[\"avgPrice-s3\"] = train[\"avgPrice\"] ** 3\n",
    "train[\"avgPrice-Sq\"] = np.sqrt(train[\"avgPrice\"])\n",
    "train[\"kijun_hb-s2\"] = train[\"kijun_hb\"] ** 2\n",
    "train[\"kijun_hb-s3\"] = train[\"kijun_hb\"] ** 3\n",
    "train[\"kijun_hb-Sq\"] = np.sqrt(train[\"kijun_hb\"])\n",
    "train[\"rosenka_hb-s2\"] = train[\"rosenka_hb\"] ** 2\n",
    "train[\"rosenka_hb-s3\"] = train[\"rosenka_hb\"] ** 3\n",
    "train[\"rosenka_hb-Sq\"] = np.sqrt(train[\"rosenka_hb\"])\n",
    "\n",
    "test[\"koji_hb-s2\"] = test[\"koji_hb\"] ** 2\n",
    "test[\"koji_hb-s3\"] = test[\"koji_hb\"] ** 3\n",
    "test[\"koji_hb-Sq\"] = np.sqrt(test[\"koji_hb\"])\n",
    "test[\"avgPrice-s2\"] = test[\"avgPrice\"] ** 2\n",
    "test[\"avgPrice-s3\"] = test[\"avgPrice\"] ** 3\n",
    "test[\"avgPrice-Sq\"] = np.sqrt(test[\"avgPrice\"])\n",
    "test[\"kijun_hb-s2\"] = test[\"kijun_hb\"] ** 2\n",
    "test[\"kijun_hb-s3\"] = test[\"kijun_hb\"] ** 3\n",
    "test[\"kijun_hb-Sq\"] = np.sqrt(test[\"kijun_hb\"])\n",
    "test[\"rosenka_hb-s2\"] = test[\"rosenka_hb\"] ** 2\n",
    "test[\"rosenka_hb-s3\"] = test[\"rosenka_hb\"] ** 3\n",
    "test[\"rosenka_hb-Sq\"] = np.sqrt(test[\"rosenka_hb\"])\n",
    "\n",
    "train[\"sum\"] = train[columns].sum(axis=1)\n",
    "test[\"sum\"] = test[columns].sum(axis=1)\n",
    "train[\"var\"] = train[columns].var(axis=1)\n",
    "test[\"var\"] = test[columns].var(axis=1)\n",
    "train[\"median\"] = train[columns].median(axis=1)\n",
    "test[\"median\"] = test[columns].median(axis=1)\n",
    "train[\"mean\"] = train[columns].mean(axis=1)\n",
    "test[\"mean\"] = test[columns].mean(axis=1)\n",
    "train[\"std\"] = train[columns].std(axis=1)\n",
    "test[\"std\"] = test[columns].std(axis=1)\n",
    "train[\"max\"] = train[columns].max(axis=1)\n",
    "test[\"max\"] = test[columns].max(axis=1)\n",
    "train[\"min\"] =train[columns].min(axis=1)\n",
    "test[\"min\"] = test[columns].min(axis=1)\n",
    "train[\"skew\"] = train[columns].skew(axis=1)\n",
    "test[\"skew\"] = test[columns].skew(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部埼玉県なので住居から消去\n",
    "# あと市が抜けてる住所情報は追加してあげる\n",
    "\n",
    "shi_gun_dic = dict({'にっさい花みず木':'坂戸市にっさい花みず木','西鶴ヶ岡':'ふじみ野市西鶴ヶ岡', \\\n",
    "                    '杉戸町内田':'北葛飾郡杉戸町内田','宮代町宮代台':'南埼玉郡宮代町宮代台', \\\n",
    "                    '大字下日出谷':'桶川市大字下日出谷','杉戸町清地':'北葛飾郡杉戸町', \\\n",
    "                    '松伏町田中':'北葛飾郡松伏町','大字水野字逃水':'狭山市大字水野字逃水'})\n",
    "\n",
    "train['jukyo'] = train['jukyo'].str.replace('埼玉県','')\n",
    "test['jukyo'] = test['jukyo'].str.replace('埼玉県','')\n",
    "train['jukyo'] = train['jukyo'].replace(shi_gun_dic)\n",
    "test['jukyo'] = test['jukyo'].replace(shi_gun_dic)\n",
    "\n",
    "jukyo_split_train = train['jukyo'].str.split(r'市|郡', n=1, expand=True)\n",
    "train['jukyo_shi_gun'] = jukyo_split_train[0]\n",
    "train.drop('jukyo', axis=1, inplace=True)\n",
    "\n",
    "jukyo_split_test = test['jukyo'].str.split(r'市|郡', n=1, expand=True)\n",
    "test['jukyo_shi_gun'] = jukyo_split_test[0]\n",
    "test.drop('jukyo', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後に、categoricalなカラムを全てone_hot_encode\n",
    "\n",
    "categorical = ['bas_toho1','bas_toho2','bokachiiki','gas','hiatari','hw_status','jigata','kodochiku','levelplan', \\\n",
    "               'road1_hk','road1_sb','road2_hk','road2_sb','road3_sb','road3_hk','road4_sb','road4_hk','road_st', \\\n",
    "               'rosen_nm1','rosen_nm2','setsudo_hi','setsudo_kj','toshikuiki1','toshikuiki2','usui','yoto1','yoto2', \\\n",
    "               'jukyo_shi_gun','level','rooms']\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(train[categorical])], axis=1)\n",
    "train.drop(categorical, axis=1, inplace=True)\n",
    "test = pd.concat([test, pd.get_dummies(test[categorical])], axis=1)\n",
    "test.drop(categorical, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 両方のDataframeに登場しないカラムを除外（価格はキープしとく）\n",
    "\n",
    "train_columns = list(train.columns.values)\n",
    "test_columns = list(test.columns.values)\n",
    "unique_columns = list(set(train_columns) ^ set(test_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold1 start\n",
      "fold1 end\n",
      "Accuracy = 9.529338739965509\n",
      "fold2 start\n",
      "fold2 end\n",
      "Accuracy = 9.835394600817677\n",
      "fold3 start\n",
      "fold3 end\n",
      "Accuracy = 9.263996602121512\n",
      "fold4 start\n",
      "fold4 end\n",
      "Accuracy = 11.645193835930904\n",
      "fold5 start\n",
      "fold5 end\n",
      "Accuracy = 10.175452093924207\n",
      "[9.529338739965509, 9.835394600817677, 9.263996602121512, 11.645193835930904, 10.175452093924207] 平均score 10.089875174551961\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train.drop(unique_columns, axis=1, inplace=True, errors='ignore')\n",
    "test.drop(unique_columns, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 3分割交差検証を指定し、インスタンス化 \n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "# スコアとモデルを格納するリスト \n",
    "score_list = [] \n",
    "models = [] \n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "for fold_, (train_index, valid_index) in enumerate(kf.split(train, y_train)):\n",
    "    train_x = train.iloc[train_index]\n",
    "    valid_x = train.iloc[valid_index]\n",
    "    train_y = y_train[train_index]\n",
    "    valid_y = y_train[valid_index]\n",
    "    \n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_x, train_y)\n",
    "    lgb_eval = lgb.Dataset(valid_x, valid_y, reference=lgb_train)\n",
    "    print(f'fold{fold_ + 1} start')\n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=5000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=0)\n",
    "    y_pred = gbm.predict(valid_x, num_iteration=gbm.best_iteration)\n",
    "    score_list.append(mean_absolute_percentage_error(valid_y, y_pred))\n",
    "    models.append(gbm)  # 学習が終わったモデルをリストに入れておく\n",
    "    print(f'fold{fold_ + 1} end\\nAccuracy = {mean_absolute_percentage_error(valid_y, y_pred)}')\n",
    "print(score_list, '平均score', np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.zeros((len(test), 5)) \n",
    "for fold_, gbm in enumerate(models):\n",
    "    pred_ = gbm.predict(test, num_iteration=gbm.best_iteration)# testを予測\n",
    "    test_pred[:, fold_] = pred_ \n",
    "pred = np.mean(test_pred, axis=1)\n",
    "first_submission['price'] = pred\n",
    "first_submission.to_csv('lightgbm.tsv', sep='\\t', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
